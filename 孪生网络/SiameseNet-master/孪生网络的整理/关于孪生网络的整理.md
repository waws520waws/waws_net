关于孪生网络的整理

这个网络主要是可以完成两张图片相似度的一个比较

#### 1.导包

```python
from tensorflow.keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, concatenate
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras import initializers

import tensorflow as tf
import numpy as np
import os
# 设置device 
os.environ["CUDA_VISIBLE_DEVICES"] = ""
import time
from dataReader import SiameseLoader
import config as cfg
```

#### 2.构建孪生网络

* 首先定义输入层
* 然后对和偏执权重进行初始化
* 使用Sequential构建模型(构建比较简单、注意下参数的对应即可)
  * 卷积层、池化层、卷积层、池化层、卷积层、池化层、卷积层、拉平、全连接层
* 将输入输入到模型中得到两个编码向量
* 输出层：对两个编码进行距离的计算，将两个编码向量合并成一个层，经过一个全连接层，使用sigmod函数进行激活
* 对上面的网络进行汇总，左边一个网络，右边一个网络，还有输出层，组合成siamese_net
* 选择优化器Adam
* 使用compile的方式完成迭代优化
* 最后返回模型

```python
def siamese_network(input_shape=(105, 105, 1), lr=0.00006):
    """
    孪生网络
    :param input_shape: 输入的shape
    :param lr: 学习率
    :return:
    """
    left_input = Input(input_shape)
    right_input = Input(input_shape)

    # 均值为0，标准差为0.01的正态分布
    w_init = initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)
    b_init = initializers.RandomNormal(mean=0.5, stddev=0.01, seed=None)

    model = Sequential([
        Conv2D(64, (10, 10), activation='relu', input_shape=input_shape,  # shape(n, 64, 94, 94)
               kernel_initializer=w_init, kernel_regularizer=l2(2e-4)),
        MaxPooling2D(),
        Conv2D(128, (7, 7), activation='relu',  # shape(n, 128, 88, 88)
               kernel_initializer=w_init, kernel_regularizer=l2(2e-4), bias_initializer=b_init),
        MaxPooling2D(),
        Conv2D(128, (4, 4), activation='relu',  # shape(n, 128, 85, 85)
               kernel_initializer=w_init, kernel_regularizer=l2(2e-4), bias_initializer=b_init),
        MaxPooling2D(),
        Conv2D(256, (4, 4), activation='relu',  # shape(n, 256, 82, 82)
               kernel_initializer=w_init, kernel_regularizer=l2(2e-4), bias_initializer=b_init),
        Flatten(),
        Dense(4096, activation="sigmoid",
              kernel_initializer=w_init, kernel_regularizer=l2(1e-3), bias_initializer=b_init)
    ])

    # 输出两个编码向量
    encoded_l = model(left_input)
    encoded_r = model(right_input)

    # 合并两个编码的L1距离
    l1_layer = Lambda(lambda x: tf.abs(x[0] - x[1]))
    l1_distance = l1_layer([encoded_l, encoded_r])
    prediction = Dense(1, activation='sigmoid', bias_initializer=b_init)(l1_distance)

    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)
    optimizer = Adam(lr=lr)

    siamese_net.compile(loss="binary_crossentropy", optimizer=optimizer)
    # siamese_net.summary()

    return siamese_net
```

#### 3.训练通过普通模式

* 传递参数
  * n_iter ：迭代次数
  * evaluate_every 多少轮进行一次验证
  * loss_every 多少轮打印一次损失
* 对于迭代次数(实际上就是epoch)进行迭代训练
* 数据的加载
  * inputs, labels = **loader.get_batch(batch_size)** 
  * 详情部分见：孪生网络的数据加载的部分
* 直接调用train_on_batch的方法进行训练
  * model.train_on_batch(inputs, labels)
  * 详情部分见：孪生网络训练的部分
* 数据的保存
  * 保存训练的损失
  * 训练的迭代次数到达一定数目的时候，进行准确率和模型的保存
    * loader.test_oneshot(model, n_way, num, verbose=True)
      * 详情见dataReader的test_oneshot的方法
    * 当我们得到的验证的正确率高于历史记录的时候，我们将这一次的模型保存下来
    * 到达loss_every的时候输出一次损失信息
* 最后将我们训练得到的训练的损失和准确率，存储在硬盘上

```python
def train_on_low_level(model, loader, batch_size, weights_path, summary_path, n_way, num):
    """
    以底层的方式训练 且 验证
    :param model: 模型架构
    :param loader: 数据读取器
    :param batch_size:
    :param weights_path: 模型保存位置
    :param summary_path: 训练曲线保存位置
    :param n_way: 测试的类别
    :param num: 测试的数量
    :return:
    """
    best_acc = 0
    evaluate_every = 10
    loss_every = 20
    # n_iter = 50
    n_iter = 1000
    train_loss = []
    valid_accs = []

    start_time = time.time()
    for i in range(1, n_iter):
        inputs, labels = loader.get_batch(batch_size)
        # 在需要对每个batch都进行处理的时候，就用train_on_batch
        loss = model.train_on_batch(inputs, labels)
        train_loss.append(loss)

        print("Loss: {:.4f}".format(loss))
        if i % evaluate_every == 0:
            print("Time for {} iterations: {:.2f}s".format(i, time.time() - start_time))
            valid_acc = loader.test_oneshot(model, n_way, num, verbose=True)
            valid_accs.append(valid_acc)

            if valid_acc >= best_acc:
                print("Current best: {}, previous best: {}".format(valid_acc, best_acc))
                print("Saving weights to: {} \n".format(weights_path))
                model.save_weights(weights_path)
                best_acc = valid_acc

        if i % loss_every == 0:
            print("iteration {}, training loss: {:.2f},".format(i, loss))

    # 把数据保存下来，服务器上不好显示，在本机显示图像
    curves_path = summary_path + "train_curves.txt"
    with open(curves_path, 'w') as f:
        f.write("[train_loss]\n")
        for loss in train_loss:
            f.write("{:.4f},".format(loss))

        f.write("\n[valid_accs]\n")
        for acc in valid_accs:
            f.write("{:.2f},".format(acc))
```

#### 4.以生成器的方式完成训练

* 我们可以将数据的加载做成生成器的方式
* 通过生成器的方式进行训练**model.fit_generator**

```python
def train_by_generator(model, loader, batch_size, epochs, weight_path):
    """
    用fit_generator来训练
    :param model: 模型架构
    :param loader: 数据读取器
    :param batch_size:
    :param epochs:
    :param weight_path: 模型保存路径
    :return:
    """
    model.fit_generator(loader.generate(batch_size, 'train'),
                        steps_per_epoch=max(1, loader.train_num // batch_size),
                        validation_data=loader.generate(batch_size, 'valid'),
                        validation_steps=max(1, loader.valid_num // batch_size),
                        epochs=epochs
                        )

    model.save_weights(weight_path)
```

#### 5.main的主体，代码进入的部分

* 首先不存在的目录该创建的要创建
* 然后加载网络
* 加载数据
* 判断是用生成器的方式进行训练，还是普通方式进行训练
  * 高手直接生成器的方式进行训练
  * 需要更加详细了解过程的可以使用普通模式训练，方便调试
* 训练即可

```python
def main():
    root_path = os.path.split(cfg.model_path)[0]
    if not os.path.exists(root_path):
        os.makedirs(root_path)

    root_path = os.path.split(cfg.summary_path)[0]
    if not os.path.exists(root_path):
        os.makedirs(root_path)

    model = siamese_network(cfg.input_shape, cfg.learning_rate)
    loader = SiameseLoader(cfg.data_path)

    if cfg.train_mode == "generator":
        train_by_generator(model, loader, cfg.batch_size, cfg.epochs, cfg.model_path)
    else:
        train_on_low_level(model, loader, cfg.batch_size, cfg.model_path, cfg.summary_path, 20, 10)
```

